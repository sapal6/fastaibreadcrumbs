# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_core.ipynb.

# %% auto 0
__all__ = ['pixel_loss', 'setup_data', 'apply_motion_blur', 'crappifier', 'show_plot', 'crappify_imgs', 'compare_imgs',
           'get_unet_dls', 'gram_matrix', 'FeatureLoss', 'non_competition_nb_meta', 'push_non_competition_notebook']

# %% ../00_core.ipynb 4
import cv2
from fastai.vision.all import *
from fastkaggle import *

# %% ../00_core.ipynb 6
def setup_data(user: str, dataset: str, install=''):
    "Get a path to data for the `user's` `dataset`, downloading it if needed"
    if iskaggle:
        if install:
            os.system(f'pip install -Uqq {install}')
        return Path('../input')/dataset
    else:
        path = Path(dataset)
        from kaggle import api
        if not path.exists():
            api.dataset_download_cli(f'{user}/{dataset}', path=str(path), unzip=True)
        return path

# %% ../00_core.ipynb 10
def apply_motion_blur(img, sz: int, angle: int):
    """apply motion blur in images at the givn angle and size of blur lines"""
    k = np.zeros((sz, sz), dtype=np.float32)
    k[ (sz-1)// 2 , :] = np.ones(sz, dtype=np.float32)
    k = cv2.warpAffine(k, cv2.getRotationMatrix2D( (sz / 2 -0.5 , sz / 2 -0.5 ) , angle, 1.0), (sz, sz) )  
    k = k * ( 1.0 / np.sum(k) )        
    return cv2.filter2D(img, -1, k)

# %% ../00_core.ipynb 13
def crappifier(f_src:Path, path_crappy: Path, sz:int=30, angle:int=0):
    """apply motion blur on images"""
    dest = path_crappy/f_src.relative_to(f_src.parent)
    dest.parent.mkdir(parents=True, exist_ok=True)
    img = cv2.imread(str(f_src))
    img = apply_motion_blur(img, sz, angle)
    cv2.imwrite(str(dest), img)

# %% ../00_core.ipynb 18
def show_plot(fn_first: Path, fn_second: Path, nrow:int, szs: tuple):
    """show two images side by side for given number of rows and figure size"""
    _, axs = plt.subplots(nrow, 2, figsize=szs)
    axs[0].set_title("original")
    axs[1].set_title("crappy")
    axs = axs.flatten()
    img_first = Image.open(fn_first)
    img_second = Image.open(fn_second)
    axs[0].imshow(img_first)
    axs[1].imshow(img_second)

# %% ../00_core.ipynb 22
def crappify_imgs(path_hr:Path, path_crappy:Path, sz=30, angle:int=0, n_workers=2):
    'parallely crappify images'
    imgs_hr = get_image_files(path_hr)
    parallel(crappifier, imgs_hr, path_crappy, path_hr, sz=sz, angle=angle, n_workers=n_workers)

# %% ../00_core.ipynb 26
def compare_imgs(origs:L, crappys:L, preds:L, szs: tuple, nrow:int=9, fontsz=100):
    """compare 3 images side by side for given number of rows and figure size"""
    if len(origs) == 1:
        raise Exception("More than one image per source needs to be available.")
    
    if len(origs) == 0:
        raise Exception("No images present in one or more of the sources.")
        
    if len(origs) < nrow:
        nrow=len(origs)
        
    _, axs = plt.subplots(nrow, 3, figsize=szs)
    print(axs)
    axs[0,0].set_title("Original", fontsize=fontsz)
    axs[0,1].set_title("Crappy", fontsize=fontsz)
    axs[0,2].set_title("Generated", fontsize=fontsz)
    [axi.set_axis_off() for axi in axs.ravel()]
    
    for i in range(nrow):
        orig=origs[i]
        crappy=crappys[i]
        pred=preds[i]
        
        orig = Image.open(orig)
        axs[i,0].imshow(orig)
        
        crappy = Image.open(crappy)
        axs[i,1].imshow(crappy)
        
        pred = Image.open(pred)
        axs[i,2].imshow(pred)

# %% ../00_core.ipynb 31
def get_unet_dls(bs:int,  source=None,
                  blocks: tuple=(ImageBlock, ImageBlock), dl_type=None, getters=None,
                  n_inp=None, get_items=None, get_y=None, get_x=None, splitter=None,
                  item_tfms=None, batch_tfms=None, **kwargs):
    """function to create the datablock and the dataloader for Unet"""
    dblock = DataBlock(blocks=blocks,
                       dl_type=dl_type,
                       getters=getters,
                       n_inp=n_inp,
                       get_items=None,
                       get_x=get_x,
                       get_y=get_y,
                       splitter=splitter,
                       item_tfms=item_tfms,
                       batch_tfms=batch_tfms)

    dls=dblock.dataloaders(source, bs=bs, **kwargs)
    dls.c=3
    
    return dls

# %% ../00_core.ipynb 36
def gram_matrix(x):
    """function to calculate the gram matrix"""
    n,c,h,w = x.size()
    x = x.view(n, c, -1)
    return (x @ x.transpose(1,2))/(c*h*w)

# %% ../00_core.ipynb 37
pixel_loss = F.l1_loss

# %% ../00_core.ipynb 38
class FeatureLoss(Module):
    """Class to calculate feature loss"""
    def __init__(self, m_feat, layer_ids, layer_wgts):
        self.m_feat = m_feat
        self.loss_features = [self.m_feat[i] for i in layer_ids]
        self.hooks = hook_outputs(self.loss_features, detach=False)
        self.wgts = layer_wgts
        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
              ] + [f'gram_{i}' for i in range(len(layer_ids))]

    def make_features(self, x, clone=False):
        self.m_feat(x)
        return [(o.clone() if clone else o) for o in self.hooks.stored]
    
    def forward(self, input, target, reduction='mean'):
        out_feat = self.make_features(target, clone=True)
        in_feat = self.make_features(input)
        self.feat_losses = [pixel_loss(input,target,reduction=reduction)]
        self.feat_losses += [pixel_loss(f_in, f_out,reduction=reduction)*w
                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
        self.feat_losses += [pixel_loss(gram_matrix(f_in), gram_matrix(f_out),reduction=reduction)*w**2 * 5e3
                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]
        if reduction=='none': 
            self.feat_losses = [f.mean(dim=[1,2,3]) for f in self.feat_losses[:4]] + [f.mean(dim=[1,2]) for f in self.feat_losses[4:]]
        for n,l in zip(self.metric_names, self.feat_losses): setattr(self, n, l)
        return sum(self.feat_losses)
    
    def __del__(self): self.hooks.remove()

# %% ../00_core.ipynb 44
def non_competition_nb_meta(user, id, title, file, dataset=None, private=True, gpu=False, internet=True):
    "Get the `dict` required for a kernel-metadata.json file"
    d = {
      "id": f"{user}/{id}",
      "title": title,
      "code_file": file,
      "language": "python",
      "kernel_type": "notebook",
      "is_private": private,
      "enable_gpu": gpu,
      "enable_internet": internet,
      "keywords": [],
      "dataset_sources": [],
      "kernel_sources": []
    }
    if dataset: d["dataset_sources"] = [f"{user}/{dataset}"]
    return d

# %% ../00_core.ipynb 47
def push_non_competition_notebook(user, id, title, file, path='.', dataset=None, private=True, gpu=False, internet=True):
    "Push notebook `file` to Kaggle Notebooks"
    meta = non_competition_nb_meta(user, id, title, file=file, dataset=dataset, private=private, gpu=gpu, internet=internet)
    path = Path(path)
    nm = 'kernel-metadata.json'
    path.mkdir(exist_ok=True, parents=True)
    with open(path/nm, 'w') as f: json.dump(meta, f, indent=2)
    from kaggle import api
    api.kernels_push_cli(str(path))
